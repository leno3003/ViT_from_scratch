{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8878db6-dca3-4382-ac70-e80251e3d51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision.datasets.mnist import MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25bddfa4-6f68-4b2a-b500-d40732867491",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f2dd9999b50>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "72c441b4-537f-46bd-a03b-3f728efa5f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = ToTensor()\n",
    "train_set = MNIST(root='./datasets', train=True, download=True, transform=transform)\n",
    "test_set = MNIST(root='./datasets', train=False, download=True, transform=transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "733305b9-e852-43dd-89fa-f5c905f8ee71",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_set, shuffle=True, batch_size=128)\n",
    "test_loader = DataLoader(test_set, shuffle=False, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "39552535-a486-4b53-a321-7e1612f730c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Device: ' + str(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "5ca3b6cc-8073-4606-952b-335b33a6279c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyViT((1, 28, 28), n_patches = 7, n_blocks = 2, hidden_d = 8, n_heads = 2, out_d = 10).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "29866b25-cd89-4735-981c-066d0783d8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 5\n",
    "LR = 0.005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "82959a62-f31b-4c8c-9254-157f32f5611c",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(model.parameters(), lr = LR)\n",
    "criterion = CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "95a62727-e311-4d47-a5dd-351ef596b524",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/5 [00:00<?, ?it/s]\n",
      "Epoch 1 in training:   0%|          | 0/469 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 1 in training:   0%|          | 1/469 [00:00<06:42,  1.16it/s]\u001b[A\n",
      "Epoch 1 in training:   0%|          | 2/469 [00:01<06:05,  1.28it/s]\u001b[A\n",
      "Epoch 1 in training:   1%|          | 3/469 [00:02<06:21,  1.22it/s]\u001b[A\n",
      "Epoch 1 in training:   1%|          | 4/469 [00:03<06:09,  1.26it/s]\u001b[A\n",
      "Epoch 1 in training:   1%|          | 5/469 [00:03<06:02,  1.28it/s]\u001b[A\n",
      "Epoch 1 in training:   1%|▏         | 6/469 [00:04<05:45,  1.34it/s]\u001b[A\n",
      "Epoch 1 in training:   1%|▏         | 7/469 [00:05<05:43,  1.34it/s]\u001b[A\n",
      "Epoch 1 in training:   2%|▏         | 8/469 [00:06<05:36,  1.37it/s]\u001b[A\n",
      "Epoch 1 in training:   2%|▏         | 9/469 [00:06<05:52,  1.30it/s]\u001b[A\n",
      "Epoch 1 in training:   2%|▏         | 10/469 [00:07<05:59,  1.28it/s]\u001b[A\n",
      "Training:   0%|          | 0/5 [00:08<?, ?it/s]                      \u001b[A\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[72], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m x, y \u001b[38;5;241m=\u001b[39m batch\n\u001b[1;32m      6\u001b[0m x, y \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(device), y\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m----> 7\u001b[0m y_hat \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(y_hat, y)\n\u001b[1;32m      9\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu() \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_loader) \u001b[38;5;66;03m# faccio la media delle loss?\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[65], line 70\u001b[0m, in \u001b[0;36mMyViT.forward\u001b[0;34m(self, images)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m# Itero sui vari blocchi del transformer\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks:\n\u001b[0;32m---> 70\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;66;03m# Estraggo solo il token CLS (che sta all'inizio di ogni sequenza)\u001b[39;00m\n\u001b[1;32m     73\u001b[0m out \u001b[38;5;241m=\u001b[39m out[:, \u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[52], line 17\u001b[0m, in \u001b[0;36mMyViTBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 17\u001b[0m     out \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmhsa\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m     out \u001b[38;5;241m=\u001b[39m out \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(out))\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[55], line 33\u001b[0m, in \u001b[0;36mMyMSA.forward\u001b[0;34m(self, sequences)\u001b[0m\n\u001b[1;32m     30\u001b[0m     seq \u001b[38;5;241m=\u001b[39m sequence[:, head \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md_head : (head \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md_head]\n\u001b[1;32m     31\u001b[0m     q, k, v \u001b[38;5;241m=\u001b[39m q_mapping(seq), k_mapping(seq), v_mapping(seq)\n\u001b[0;32m---> 33\u001b[0m     attention \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msoftmax(\u001b[43mq\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m \u001b[38;5;241m/\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md_head \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.5\u001b[39m))\n\u001b[1;32m     34\u001b[0m     seq_result\u001b[38;5;241m.\u001b[39mappend(attention \u001b[38;5;241m@\u001b[39m v)\n\u001b[1;32m     35\u001b[0m result\u001b[38;5;241m.\u001b[39mappend(torch\u001b[38;5;241m.\u001b[39mhstack(seq_result))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training\n",
    "for epoch in trange(N_EPOCHS, desc='Training'):\n",
    "    train_loss = 0.0\n",
    "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch + 1} in training\", leave=False):\n",
    "        x, y = batch\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        y_hat = model(x)\n",
    "        loss = criterion(y_hat, y)\n",
    "        train_loss += loss.detach().cpu() / len(train_loader) # faccio la media delle loss?\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{N_EPOCHS} loss: {train_loss:.2f}\")\n",
    "return train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8656546d-b448-4f99-9881-9a69c734990f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "with torch.no_grad():\n",
    "    correct, total = 0, 0\n",
    "    test_loss = 0.0\n",
    "    for batch in tqdm(test_loader, desc='Testing'):\n",
    "        x, y = batch\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        y_hat = model(x)\n",
    "\n",
    "        loss = criterion(y_hat, y)\n",
    "        test_loss += loss.detach().cpu() / len(test_loader)\n",
    "\n",
    "        correct += torch.sum(torch.argmax(y_hat, dim=1) == y).detach().cpu().item()\n",
    "        total += len(x)\n",
    "    print(f\"Test loss: {test_loss:.2f}\")\n",
    "    print(f\"Test accuracy: {correct / total * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1b0bd6a7-4867-4ce4-9484-5d1909f94ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Devo passare da avere immagini che hanno dimensione (N, C, H, W), \n",
    "# rispettivamente: numero di elementi nel batch, canali, altezza e \n",
    "# larghezza dell'immagine, ad oggetti di dimensione (N, N_PATCHES, PATCHES_DIM)\n",
    "# Il numero di patch (N_PATCHES) è parametro del modello.\n",
    "# Il reshape sarà: (N, PxP, HxC/P x WxC/P), dove P è il numero di patch su una \n",
    "# sola dimensione, quindi il numero totale di patches sarà PxP. La dimensione delle\n",
    "# patch su ogni dimensione sarà H/P e W/P, ed ognuna di esse dovrà essere moltiplicata\n",
    "# per il numero di canali dell'immagine. La PATCHES_DIM sarà il prodotto di questi \n",
    "# due valori\n",
    "\n",
    "def patchify(images, n_patches):\n",
    "    # n_patches è il numero di patch su una dimensione dell'immagine\n",
    "    n, c, h, w = images.shape\n",
    "\n",
    "    assert h == w, \"Images must be squares\"\n",
    "\n",
    "    patches = torch.zeros(n, n_patches ** 2, h*c*w // n_patches ** 2) # Cos'è l'ultima dimensione?\n",
    "    patch_size = h // n_patches\n",
    "\n",
    "    for idx, image in enumerate(images):\n",
    "        for i in range(n_patches):\n",
    "            for j in range(n_patches):\n",
    "                # rompo le immagini in patch (la prima dimensione è il numero di canali, li prendo tutti)\n",
    "                # facendo scorrere i vari patch sopra l'immagine.\n",
    "                patch = image[:, i*patch_size : (i+1) * patch_size, j*patch_size : (j+1) * patch_size]\n",
    "                # salvo le patch schiacciate in una matrice, rispecchiando\n",
    "                # nella matrice le posizioni che le patch avevano nell'immagine\n",
    "                patches[idx, i*n_patches+j] = patch.flatten()\n",
    "\n",
    "    return patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "33dcbd67-29a6-4719-8e4e-44b797f56dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_positional_embeddings(sequence_length, d):\n",
    "    result = torch.ones(sequence_length, d)\n",
    "    for i in range(sequence_length):\n",
    "        for j in range(d):\n",
    "            result[i][j] = np.sin(i / (10000 ** (j / d))) if j % 2 == 0 else np.cos(i / (10000 ** ((j - 1) / d)))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "dffe2493-4928-4dbb-90fc-14a957bd6a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyViT(nn.Module):\n",
    "    def __init__(self, chw = (1, 28, 28), n_patches = 7, n_blocks=2, hidden_d=8, n_heads=2, out_d=10):\n",
    "        super(MyViT, self).__init__()\n",
    "        \n",
    "        # Attributi\n",
    "        self.chw = chw\n",
    "        self.n_patches = n_patches\n",
    "        self.hidden_d = hidden_d\n",
    "        self.n_blocks = n_blocks\n",
    "        self.n_heads = n_heads\n",
    "        \n",
    "        self.patch_size = (chw[1] / n_patches, chw[2] / n_patches)\n",
    "        \n",
    "        # Primo mapping lineare: necessario perchè non posso dare in input\n",
    "        # al transformer un patch a due dimensioni. Uso una trasformazione lineare\n",
    "        # che mi porta da una matrice 4x4 (singolo patch) ad un mapping lungo 8.\n",
    "        # La dimensione di output del mapping è arbitraria e fissa.\n",
    "        \n",
    "        # Dimensione in input per la trasformazione lineare, in questo caso è 16 (4x4)\n",
    "        self.input_d = int(chw[0] * self.patch_size[0] * self.patch_size[1])\n",
    "        \n",
    "        # Trasformazione lineare (è solo una matrice)\n",
    "        self.linear_mapper = nn.Linear(self.input_d, self.hidden_d)\n",
    "        \n",
    "        # Aggiungo il token CLS alla sequenza: è un token posto all'inizio della sequenza dei\n",
    "        # patch schiacciati e trasformati linearmente. Voglio che, durante il training,\n",
    "        # a tale token venga 'assegnata' l'informazione globale contenuta in tutti gli\n",
    "        # altri patch della sequenza, in modo tale da poter fare classificazione.\n",
    "        # Il valore di tale token deve essere apprendibile dal modello, per questo\n",
    "        # è di tipo nn.Parameter (ovvero un tensore che viene aggiunto al modello e\n",
    "        # su cui calcolo la discesa del gradiente affinchè contenga l'informazione\n",
    "        # necessaria al task che voglio che faccia). Molto interessante: posso far fare\n",
    "        # ai vari token i task che voglio, fintanto che sono di tipo nn.Parameter\n",
    "        # perchè il training farà in modo che apprendano l'informazione necessaria al\n",
    "        # task che gli assegno.\n",
    "        \n",
    "        self.class_token = nn.Parameter(torch.rand(1, self.hidden_d)) # Inizializzato a caso.\n",
    "        \n",
    "        # Aggiungo i positional embeddings\n",
    "        # self.pos_embed = nn.Parameter(torch.Tensor(get_positional_embeddings(self.n_patches ** 2 + 1, self.hidden_d)))\n",
    "        # self.pos_embed.requires_grad = False # Uso solo seno e coseno per calcolarli, non sono valori da apprendere\n",
    "        self.register_buffer('positional_embeddings', get_positional_embeddings(n_patches ** 2 + 1, hidden_d), persistent=False)\n",
    "        \n",
    "        # Blocchi di encoder\n",
    "        self.blocks = nn.ModuleList([MyViTBlock(hidden_d, n_heads) for _ in range(n_blocks)])\n",
    "        \n",
    "        # MLP per la classificazione\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(self.hidden_d, out_d),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, images):\n",
    "        \n",
    "        n, c, h, w = images.shape\n",
    "        patches = patchify(images, self.n_patches).to(self.positional_embeddings.device)\n",
    "        tokens = self.linear_mapper(patches)\n",
    "        \n",
    "        # Aggiungo il CLS alla sequenza\n",
    "        # tokens = torch.stack([torch.vstack((self.class_token, tokens[i])) for i in range(len(tokens))])\n",
    "        tokens = torch.cat((self.class_token.expand(n, 1, -1), tokens), dim=1)\n",
    "        \n",
    "        # Aggiungo ai token l'informazione posizionale\n",
    "        # pos_embed = self.pos_embed.repeat(images.shape[0], 1, 1)\n",
    "        # out = tokens + pos_embed\n",
    "        out = tokens + self.positional_embeddings.repeat(n, 1, 1)\n",
    "        \n",
    "        # Itero sui vari blocchi del transformer\n",
    "        for block in self.blocks:\n",
    "            out = block(out)\n",
    "        \n",
    "        # Estraggo solo il token CLS (che sta all'inizio di ogni sequenza)\n",
    "        out = out[:, 0]\n",
    "        \n",
    "        return self.mlp(out) # faccio classificazione su tale token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7ec5b2ea-a59d-48ae-8870-f783333945e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyMSA(nn.Module):\n",
    "    def __init__(self, d, n_heads = 2):\n",
    "        super(MyMSA, self).__init__()\n",
    "        self.d = d\n",
    "        self.n_heads = n_heads\n",
    "        \n",
    "        d_head = int(d / n_heads)\n",
    "        \n",
    "        # Tutte e tre K, Q, V sono matrici, dunque trasformazioni lineari\n",
    "        self.q_mappings = nn.ModuleList([nn.Linear(d_head, d_head) for _ in range(self.n_heads)])\n",
    "        self.k_mappings = nn.ModuleList([nn.Linear(d_head, d_head) for _ in range(self.n_heads)])\n",
    "        self.v_mappings = nn.ModuleList([nn.Linear(d_head, d_head) for _ in range(self.n_heads)])\n",
    "        \n",
    "        self.d_head = d_head\n",
    "        self.softmax = nn.Softmax(-1)\n",
    "        \n",
    "    def forward(self, sequences):\n",
    "        # La sequenza di token ha shape (N, seq_length, token_dim)\n",
    "        # La portiamo alla shape (N, seq_length, n_heads, token_dim / n_heads)\n",
    "        # E poi la riportiamo a \n",
    "        \n",
    "        result = []\n",
    "        for sequence in sequences:\n",
    "            seq_result = []\n",
    "            for head in range(self.n_heads):\n",
    "                q_mapping = self.q_mappings[head]\n",
    "                k_mapping = self.k_mappings[head]\n",
    "                v_mapping = self.v_mappings[head]\n",
    "                \n",
    "                seq = sequence[:, head * self.d_head : (head + 1) * self.d_head]\n",
    "                q, k, v = q_mapping(seq), k_mapping(seq), v_mapping(seq)\n",
    "                \n",
    "                attention = self.softmax(q @ k.T / (self.d_head ** 0.5))\n",
    "                seq_result.append(attention @ v)\n",
    "            result.append(torch.hstack(seq_result))\n",
    "            \n",
    "        return torch.cat([torch.unsqueeze(r, dim=0) for r in result])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d3dbab72-c960-4f88-a0dc-9c22cdca2664",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyViTBlock(nn.Module):\n",
    "    def __init__(self, hidden_d, n_heads, mlp_ratio=4):\n",
    "        super(MyViTBlock, self).__init__()\n",
    "        self.hidden_d = hidden_d\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(hidden_d)\n",
    "        self.mhsa = MyMSA(hidden_d, n_heads)\n",
    "        self.norm2 = nn.LayerNorm(hidden_d)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(hidden_d, mlp_ratio * hidden_d),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(mlp_ratio * hidden_d, hidden_d)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x + self.mhsa(self.norm1(x))\n",
    "        out = out + self.mlp(self.norm2(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "33a030c8-89b7-4569-a8c5-bb5ff15dec4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7, 50, 8])\n"
     ]
    }
   ],
   "source": [
    "model = MyViTBlock(hidden_d=8, n_heads=2)\n",
    "\n",
    "x = torch.randn(7, 50, 8)  # Dummy sequences\n",
    "print(model(x).shape)      # torch.Size([7, 50, 8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8067e806-3fef-4afe-a367-8ea38a136661",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7, 10])\n"
     ]
    }
   ],
   "source": [
    "model = MyViT(chw=(1, 28, 28), n_patches=7)\n",
    "x = torch.randn(7, 1, 28, 28)\n",
    "print(model(x).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a057179c-c788-43ed-89b6-4fe7b5c4b6c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
